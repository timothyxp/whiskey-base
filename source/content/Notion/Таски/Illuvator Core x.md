---
Created: 2022-10-31T22:53
---
  

  

  

|   |   |   |   |   |   |   |   |   |   |   |   |
|---|---|---|---|---|---|---|---|---|---|---|---|
|Model size|seq_len|batch|TF+xla, lluvator, samples/sec|TF, lluvator, samples/sec|TF+fp16, lluvator, samples/sec|TF, V100, samples/sec|TF+fp16, V100, samples/sec|TF+fp16+xla, V100, samples/sec|our TF, V100, samples/sec|our TF+fp16, V100, samples/sec|our TF+fp16+xla, V100, samples/sec|
|base|128|128|[seg fault](https://paste.yandex-team.ru/794a8043-f9d7-4a18-a748-85daa5649828)|110|139|120|225|363|120|255|506|
|base|128|160|OOM|OOM|143|OOM|230|375|121|266|548|
|base|128|192|OOM|OOM|OOM|OOM|OOM|OOM|121|271|555|
|||||||||||||
|base|256|32||47|55|54|88|147||||
|base|256|48||OOM|60||96||54|109||
|base|256|64||OOM|62||100||56|116||
|||||||||||||
|base|256|32 x 2||80|85|98|155|||||
|base|256|48 x 2|||97||172|||||
|base|256|64 x 2|||104||184|||||

cuda10.2 vs cuda 11.0

  

## Тикет

Для тестирования взяли предложенный ими код берта, по сути довольно близок к оригинальному, но с патчами от nvidia, буду обозначать nvidia-bert. Наш берт - bert. Наша реализация довольно близка к их реализации, но сравнивать на ней нельзя, тк заносить наш код на их сервер не хочется. Поэтому основной бенчмарк будет на nvidia-bert, но будем и смотреть на нашу чтоб понимать отставание. Теоретически все наши фишки можно и занести в их код, но не в рамках данного бенчмарка.

Тестировать будем fine-tune на датасете MNLI([https://yt.yandex-team.ru/hahn/navigation?navmode=content&path=//home/factordev/timothyxp/glue/mnli/train](https://yt.yandex-team.ru/hahn/navigation?navmode=content&path=//home/factordev/timothyxp/glue/mnli/train)) из GLUE.

Гиперпараметры тут одинаковые:

- batch_size=seq_len=128
- model_size=base

|Configuration|fp32, samples/sec|fp16, samples/sec|xla+fp16, samples/sec|
|---|---|---|---|
|nvidia-bert + Illuvatar|110|139|[seg fault](https://paste.yandex-team.ru/794a8043-f9d7-4a18-a748-85daa5649828)|
|nvidia-bert + V100|120|225|363|
|bert + V100|120|255|506|

Можно отметить, что

- В 32-х флотах все конфигурации очень схожи, Illuvatar отстает на 10%
- С 16-ми флотами у Iluvatar какие-то проблемы, ускорение довольно слабое(20%)
- XLA в нашей конфигурации дает больше буста, чем у nvidia-bert, но у них он не запускается. Спросил их по поводу поддержки XLA, сказали что сейчас реализовывают его поддержку под TensorFlow2 для CV-шных сеток, для бертов пока не планируют(можно их об этом попросить).  
      
    

Далее так как xla не поддерживается буду бенчмаркать nvidia-bert на 32-х и 16-х флотах для разных seq_len, batch_size.

|seq_len|batch_size|Iluvatar + fp32|Iluvatar + fp16|V100 + fp32|V100 + fp16|
|---|---|---|---|---|---|
|128|128|110|139|120|225|
|128|160|OOM|143|OOM|230|
|||||||
|256|32|47|55|54|88|
|256|48|OOM|60|OOM|96|
|256|64|OOM|62|OOM|100|

Картина в целом схожая по разным бачам, входным длинам.

  

Также замерим scalability обучения на 2-х гпу, samples/sec(scalability)

|seq_len|batch_size|Iluvatar + fp32|Iluvatar + fp16|V100 + fp32|V100 + fp16|
|---|---|---|---|---|---|
|256|32|80(0.85)|85(0.77)|98(0.9)|155(0.88)|
|256|48|OOM|97(0.8)|OOM|172(0.89)|
|256|64|OOM|104(0.84)|OOM|185(0.92)|

В целом скейлится нормально, но чуть хуже чем у нас. Возможно я использовал не самый оптимальный метод(reduct-to-one) для их конфигурации.

  

В итоге:

- в 32-х флотах получаем схожее поведение с V100
- с 16-ми флотами проблемы, выясняю, возможно я что-то не так сделал
- xla теоретически будут поддерживать в будущем
- скейлится в рамках одного хоста на 2-х картах приемлимо
- Учитывая, что А100 в 2-3 раза быстрее V100, текущая максимальная скорость обучения в 7-8 раз меньше чем на А100 и в 3-4 раза чем на V100
- Также отмечу, что памяти у карточки 32Гб, большие модели не поучишь

  

  

планы сделать

- [x] запустить их код на V100
- [x] дождаться про xla ответ
- [ ] спросить про float 16 почему так плохо
- [x] scalability

планы не сделать:

- grad accum