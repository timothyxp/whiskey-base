---
Created: 2022-09-26T11:33
---
**Планы по mindspore**

  

- [x] Запустить что-то(раздебажить окружение, дотаяет, докер и тд)
- [x] Привести superglue/boolq в tfrecord формат
- [x] Разобраться с запуском нашего файн-тьюна на дотасете
- [x] Разобраться с запуском их файн-тьюна на датасете
- [x] Разобраться как веса конвертить bert-base в наш
- [x] Свести forward(прикрутить логирование к mindspore)
- [x] Разобраться с запуском на их сервере
- [x] Замер скорости и качество файт-тьюна
- [x] запуск с fused операциями
- [x] посчитать с аккумуляцией че получается
- [x] посчитать для другой модельки(large)

Факты:

- Замер на публичном датасете файн-тьюна
- База - публичный bert-base, large, xlarge
- Запуски локально для честности сравнения
- Переборка ytf для работы fused lamb in progress

Важные гиперпараметры для бенчмарка

- use_xla
- batch_size
- seq_len
- n_gpu
- Grad_accum/virtual_batch_size

  

Свод:

- [x] ембеды ворд
- [x] весь эмбедер
- [x] аттеншн слои
- [x] пулеры и итог

  

Интересные факты:

- На виртуалке с А100 с cuda=11.4 под cuda10.3:ms1.3 не запускалось, на V100 c cuda 11.1 все норм

## бенчмарки

|Модель|кол-во карт|batch_size|seq_len|grad_accum|V100+tf(example/sec)|V100+tf+xla(examples/sec) float32|V100+tf+xla(example/sec)|A100+tf+xla+fp16(example/sec)|V100+mindspore+fp16(example/sec)|V100+mindspore+xla(examples/sec, warmup)|A100+mindspore+xla+fp16(example/sec)|A100+mindspore+xla(examples/sec, warmup)|NPUa910+mindspore1.5(examples/sec)|доп инфа|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|bert-base|1|32|512|1|53||110|||25/222s||||cuda=11.1, mindspore=1.7|
|bert-base|1|32|512|1|53||110|||29.7/226s|||184|cuda=10.1, mindspore=1.3|
|bert-base|1|32|256|1|||||||||313|cuda=10.1, mindspore=1.3|
|bert-base|1|32|**128**|1|140|125|250|460|200|98/222s|[`error`](https://paste.yandex-team.ru/3b3ad396-e1b1-4110-8261-4cebbf215c2b)*|`[error](https://paste.yandex-team.ru/3b3ad396-e1b1-4110-8261-4cebbf215c2b)`*|525|cuda=10.1, mindspore=1.3|
|bert-base|1|32|**128**|1|140|125|250||152|88/200s|error*|error*||cuda=10.1, mindspore=1.7|
|bert-base|1|32|**128**|1|140|125|250||151|89/266s|206/200s|121/266s||cuda=11.1, mindspore=1.7|
||||||||||||||||
||||||||||||||||

*-на сервере с А100 CUDA=11.4, на нашем 11.1.

  

## Тикет

Для тестирования выбрали открытый датасет [BoolQ](https://huggingface.co/datasets/boolq) из SuperGLUE, задача сводится к классификации.

Написал конвертатор весов и свел forward на нашем берте и mindspore реализации.

Кривые обучения при этом не мачатся, но качество +- одинаковое выходит, это еще проверю дополнительно позже.

  

**Первая** **таблица** в одной конфигурации модели, чтобы в целом понимать с какой скоростью работают разные конфигурации фреймворков

Конфигураций модели:

- seq_len =128, длина входной последовательности
- batch_size =32, размер бача
- mindspore=1.7 локально, 1.5 на их сервере
- grad_accum=1, кол-во аккумуляций градиента

Конфигурации фреймворков:

- xla - используется ли xla компиляция графа в tensorflow, для mindspore всегда использовал режим компиляции, однако он занимает больше нашего xla, добавил колонку с временем затраченым на компиляцию mindspore
- sink=1, по умолчанию(в их реализации берта) не дергается синхронизация с питоном каждый шаг. То есть граф компилируется и прогоняется 200(условно) раз подряд без синхронизаций с питонячим кодом и дергания колбэков. В нашем же коде на tf синхронизация и колбэки(запись логов, метрик…) дергаются каждый шаг, поэтому для честности сравнения включил опцию синхронизации и в mindspore, это занимает порядка 6мс на бач и замедляет немного(9%) обучение.
- В нашем реализации также есть fused операции, при их добавлении получается ускорение еще порядка 4%, в частности FusedLAMB([3%](https://pulsar.yandex-team.ru/experiments/171991d6-5c4d-415a-90d6-612777c9bedc)), Fused LayerNorm(+1%). Их также включал

|№|конфигурация запуска|examples/sec|warmup|
|---|---|---|---|
|1|V100+tf|140|0|
|2|V100+tf+xla|125|20|
|3|V100+tf+xla+fp16|**255**|20|
|4|A100+tf+xla+fp16|**483**|20|
|5|V100+mindspore|89|240|
|6|V100+mindspore+fp16|150|240|
|7|A100+mindspore|121|240|
|8|A100+mindspore+fp16|206|240|
|9|NPUa910+mindspore+fp16|525|120|
|10|NPUa910+mindspore+fp16, sink=1|**480**|120|

  

**Вторая таблица.**

Тут будем тестировать конфигурации фреймворков 3,4,10 на разных конфигурациях моделей.

Перебираем seq_len, в последнем столбце разница Ascend c TF

|Модель|кол-во карт|batch_size|seq_len|grad_accum|V100+tf+xla(example/sec)|A100+tf+xla+fp16(example/sec)|NPUa910+mindspore(examples/sec)|
|---|---|---|---|---|---|---|---|
|bert-base|1|32|512|1|112|219|176(-20%)|
|bert-base|1|32|256|1|182|319|291(-9%)|
|bert-base|1|32|128|1|255|483|480(-0.7%)|
|bert-Large|1|12|512|1|32|59|59(0)|
|bert-Large|1|24|256|1|69|127|133(+5%)|
|bert-Large|1|32|128|1|119|208|220(+6%)|
|||||||||

  

**Третья таблица.**

Перебираем batch_size

|Модель|кол-во карт|batch_size|seq_len|grad_accum|V100+tf+xla(example/sec)|A100+tf+xla+fp16(example/sec)|NPUa910+mindspore1.5(examples/sec)|
|---|---|---|---|---|---|---|---|
|bert-base|1|16|512|1|86|162|126|
|bert-base|1|32|512|1|112|219|176|
|bert-base|1|64|512|1|OOM|OOM*|172|
|bert-L**arge**|1|8|512|1|26|48|50|
|bert-L**arge**|1|12|512|1|32|59|59|
|bert-L**arge**|1|16|512|1|OOM|OOM*|62|

*-карточки А-100 40, а не 80-гиговые

Хотел также бенчмаркать количество используемой памяти, но и у Ascend и у TF не показывается реально затрачиваемая память, она при всех запусках была одинаковая. Видимо Ascend как и TF ест больше нужной ему реально памяти.

При этом у А100 все же 80Гб в большой конфигурации, а у Ascend 32. То есть большие модели учить на нем будет хуже чем на А100

  

### Четвертая таблица

Пробуем аккумулировать градиенты. В mindspore это работает чуть эффективнее за счет переходи в sink_mode режим(все шаги что не нужно гонять оптимайзер можно скипать синхронизацию с питоном и гонять граф подряд grad_aссum раз)

|Модель|кол-во карт|batch_size|seq_len|grad_accum|V100+tf+xla(example/sec)|A100+tf+xla+fp16(example/sec,)|NPUa910+mindspore1.5(examples/sec)|
|---|---|---|---|---|---|---|---|
|bert-base|1|32|512|1|112|219|176|
|bert-base|1|32|512|2|115(+3%)|226(+3%)|190(+8%)|
|bert-base|1|32|512|4|122(+9%)|237(+8%)|197(+12%)|
|bert-base|1|32|512|8|123(+10%)|234(+7%)|201(+14%)|
|bert-base|1|32|512|16|125(+12%)|238(+9%)|203(+15%)|
|bert-Large|1|12|512|1|32|59|59|
|bert-Large|1|12|512|2|OOM|61(+3%)|69(+17%)|
|bert-Large|1|12|512|4|OOM|67(+13%)|75(+27%)|
|bert-Large|1|12|512|8|OOM|70(+19%)|77(+31%)|
|bert-Large|1|12|512|16|OOM|71(+20%)|79(+35%)|

Таким образом при аккумуляции(возьмем 8 для примера) отставание mindspore 20→14% на base модели. На модели Large mindspore от равенства переходит к ускорению на 10%.