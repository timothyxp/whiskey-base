---
Created: 2022-11-15T22:27
---
|                                                    |                    |                  |              |                                                                                                |       |          |            |       |              |               |       |                                              |
| -------------------------------------------------- | ------------------ | ---------------- | ------------ | ---------------------------------------------------------------------------------------------- | ----- | -------- | ---------- | ----- | ------------ | ------------- | ----- | -------------------------------------------- |
| Model                                              | Optimizer          | Norm technic     | loss         | dataset                                                                                        | depth | hidden   | activation | fp    | pos encoding | optimizations | batch | comment                                      |
| [GLM-130B](https://arxiv.org/pdf/2210.02414.pdf)   | AdamW              | Deep-Norm+EGS    | glm          | Pile+LAMBADA+dedup                                                                             | 70    | 12k      | GeGLU      | fp16  | RoPE         | ZERO          | 4k    |                                              |
| [BLOOM-176B](https://arxiv.org/pdf/2211.05100.pdf) | AdamW              | Pre-LN+EmbLN     | gpt          | ROOTS(500 hf datasets)+dedup                                                                   | 70    | 14k      | gelu       | bf16  | ALiBi        | ZERO-3D       | 2k    |                                              |
| OPT-175B                                           | AdamW              | Pre-LN           | gpt          | roberta+pile+pushshift+dedup. рекомендация обязательно дедупать PILE                           | 96    | 12k      | relu       | fp16  | APE          | ZERO-3D       | 2M    |                                              |
| GPT-3 175B                                         | AdamW              | Pre-LN           | gpt          | CC+Wiki+…                                                                                      | 96    | 12k      | gelu       | fp16  | APE          | ZERO-         | 3.2M  |                                              |
| PaLM-540B                                          | Adam+adaptic betas | Pre-LN           | gpt          | 780В(soc+web+books+git+wiki+news)                                                              |       |          | SwiGLU     | bf16  | RoPE         | DP+MP         | 2к    |                                              |
| RuLALBERT-2B                                       |                    | Pre-LN+Smallinit | albert       | rus+dedup                                                                                      |       |          | GeGLu      |       | RoPE         | albert+custom |       |                                              |
| T5—13B                                             | adam?              | custom           | enc-dec+mlm  | mC4+line-dedup                                                                                 | 48    | 1024,65k | gelu       | fp16? | relu         | DP+MP         |       |                                              |
| GPT-NeoX-20B                                       | adamW              | custom+smallinit | gpt          | [PILE](https://arxiv.org/pdf/2101.00027.pdf), интересное рассуждение про важность дедупликации | 44    | 6144,x4  | gelu       | fp16  | RoPE         | ZERO          | 12k   | без wt                                       |
| YALM-100B                                          | LAMB               | Pre-LN           | gpt          | custom-ru                                                                                      |       |          |            | bf16  |              | ZERO          |       |                                              |
| Deberta-v2-1.3B                                    | AdamW              |                  | electra, rtd |                                                                                                | 48    | 1536     | gelu?      |       |              | ZERO          |       |                                              |
| METRO-LM-5.4B                                      | Adam?              | Post-LN          | electra, rtd |                                                                                                | 64    | 2560     | gelu?      | fp16  | relpos       | ZERO-3D       | 2k    |                                              |
| Chinchilla-70B                                     | AdamW              |                  | gpt          | MassiveWeb(1.4T)                                                                               | 80    | 8192     | gelu?      | bf16  | ?            |               | 3M    | 1.4T токенов в обучении vs 200-400B у других |
| Llama                                              |                    |                  |              |                                                                                                |       |          | swiglu     | bf16  | relpos       | zero-3d       |       | 1.4T                                         |